{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "dtype = torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['te', 'thr', 'power']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['te','thr','power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv',header=1)\n",
    "df=df.drop(columns=['thr','te'])\n",
    "current_target='power'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "x = df.drop(columns=current_target)\n",
    "y = df[current_target]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=59) #59 power, 80 thr, 70 te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for n in range(100):\n",
    "    df = df.sort_index()\n",
    "    x = df.drop(columns=current_target)\n",
    "    y = df[current_target]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=n)\n",
    "    model=CatBoostRegressor(verbose=False)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(f\"{n},{mse(model.predict(x_test),y_test)}\")\n",
    "\n",
    "1. Power\n",
    "0,11.41076332145832\n",
    "1,14.318056318234506\n",
    "2,12.615432677258951\n",
    "3,10.9692100087263\n",
    "4,10.801517687064011\n",
    "5,12.931889573120856\n",
    "6,11.165391570447097\n",
    "7,12.16917434260467\n",
    "8,10.917127820213212\n",
    "9,13.57935839615522\n",
    "10,12.432077945680346\n",
    "11,12.732707904845228\n",
    "12,11.0327850528215\n",
    "13,11.352214648250026\n",
    "14,12.481151958841847\n",
    "15,10.153587045462821\n",
    "16,11.763529866459137\n",
    "17,11.574598214644269\n",
    "18,11.86273517180898\n",
    "19,12.820310809775904\n",
    "20,11.584763546706546\n",
    "21,13.044937043699003\n",
    "22,11.758697121516974\n",
    "23,11.507046552519425\n",
    "24,12.619511216698237\n",
    "25,12.444032479758212\n",
    "26,12.83678085135937\n",
    "27,12.16733444132914\n",
    "28,10.477083000524711\n",
    "29,10.13846558623491\n",
    "30,10.769794438586501\n",
    "31,14.325811735855702\n",
    "32,11.96672275368764\n",
    "33,13.465696966404627\n",
    "34,12.083252253261369\n",
    "35,11.216962449702478\n",
    "36,12.31491864162777\n",
    "37,12.705542868757869\n",
    "38,13.444925737482796\n",
    "39,12.488122783940815\n",
    "40,10.693394400966886\n",
    "41,10.911616612947489\n",
    "42,14.900715718684118\n",
    "43,11.083119052178247\n",
    "44,11.601977449220461\n",
    "45,13.053408411244328\n",
    "46,9.899147141321839\n",
    "47,13.39057280005873\n",
    "48,10.766648703492976\n",
    "49,10.980290294937715\n",
    "50,11.153880477145165\n",
    "51,14.606199682745059\n",
    "52,11.213925566348417\n",
    "53,9.542522717059787\n",
    "54,13.567374737730795\n",
    "55,10.164773511928361\n",
    "56,11.105287654265268\n",
    "57,13.718719052701877\n",
    "58,11.333540670353608\n",
    "59,8.497001911326796\n",
    "60,11.592424241690653\n",
    "61,13.5395966025784\n",
    "62,13.852712974886401\n",
    "63,11.423705747028754\n",
    "64,10.710872019194232\n",
    "65,12.743530395528891\n",
    "66,11.573257758357832\n",
    "67,10.84146119086138\n",
    "68,10.467392989379448\n",
    "69,9.922823143106037\n",
    "70,9.493745530032145\n",
    "71,11.27202842133312\n",
    "72,11.694136676152718\n",
    "73,12.221674327283248\n",
    "74,15.035725746620779\n",
    "75,12.914080086013572\n",
    "76,11.991917552718007\n",
    "77,10.672710500953265\n",
    "78,12.553205170616755\n",
    "79,11.502409588056507\n",
    "80,10.990264842503139\n",
    "81,10.701090721830576\n",
    "82,13.345973717809267\n",
    "83,12.270014988052035\n",
    "84,10.698777741688778\n",
    "85,14.022644787847042\n",
    "86,14.003823575109015\n",
    "87,13.484354357882074\n",
    "88,16.74424161345748\n",
    "89,9.0584365100587\n",
    "90,11.301090321941386\n",
    "91,12.863339937586238\n",
    "92,11.337603778800208\n",
    "93,12.967600159536852\n",
    "94,12.614455759261125\n",
    "95,12.738102727813791\n",
    "96,14.523567952695963\n",
    "97,12.113701126343665\n",
    "98,12.909167771026341\n",
    "99,10.18505805373564\n",
    "\n",
    "2. thr\n",
    "0,4724.232213861554\n",
    "1,4610.825216184579\n",
    "2,6055.330869032593\n",
    "3,5114.488568205936\n",
    "4,4738.928403766669\n",
    "5,5314.402333946792\n",
    "6,4669.131045370651\n",
    "7,5643.249412000004\n",
    "8,5104.478731674154\n",
    "9,5528.6730198652\n",
    "10,4562.4420540945775\n",
    "11,5578.509452385982\n",
    "12,4427.96414514507\n",
    "13,5267.3296545328785\n",
    "14,5430.059025260408\n",
    "15,5339.038242094627\n",
    "16,4463.802918118435\n",
    "17,5423.283793005328\n",
    "18,5614.826095396864\n",
    "19,5092.581278663082\n",
    "20,5402.7800971575025\n",
    "21,4948.794864466815\n",
    "22,5393.849578325779\n",
    "23,4550.629594567171\n",
    "24,4528.144235806636\n",
    "25,4805.273713130066\n",
    "26,5320.834926601629\n",
    "27,5805.092561170168\n",
    "28,5120.893517645038\n",
    "29,5160.893511994251\n",
    "30,5028.491613660232\n",
    "31,5184.732443553403\n",
    "32,4867.605548440525\n",
    "33,5182.0971366363765\n",
    "34,4642.394448333459\n",
    "35,5416.064049485922\n",
    "36,5710.881021144854\n",
    "37,4770.765487367637\n",
    "38,4764.385871061131\n",
    "39,5448.845713328467\n",
    "40,6343.888120215704\n",
    "41,4764.415809691513\n",
    "42,5362.37910733152\n",
    "43,6248.051196241974\n",
    "44,5555.349469577085\n",
    "45,6074.959778640582\n",
    "46,4936.474122027513\n",
    "47,5160.211503713981\n",
    "48,5004.150353288729\n",
    "49,5066.024879125629\n",
    "50,4591.952487347261\n",
    "51,4960.987063514835\n",
    "52,5556.015570281714\n",
    "53,5431.919631876493\n",
    "54,5024.144923284437\n",
    "55,5246.384037513113\n",
    "56,5592.080987565693\n",
    "57,5218.366282843651\n",
    "58,6106.796134887011\n",
    "59,4343.505385016025\n",
    "60,4893.600718824523\n",
    "61,5367.805360770601\n",
    "62,4560.21509775607\n",
    "63,4412.1453051095505\n",
    "64,4932.0284364170375\n",
    "65,4905.036682250197\n",
    "66,4266.206574155548\n",
    "67,4761.769036425087\n",
    "68,5043.469324060581\n",
    "69,5200.119529082677\n",
    "70,4758.819409553396\n",
    "71,5371.898288429307\n",
    "72,4947.394348120425\n",
    "73,5203.300931651352\n",
    "74,5038.537048240991\n",
    "75,4448.045517703519\n",
    "76,5673.410203217506\n",
    "77,5047.3068752314175\n",
    "78,5238.677834437368\n",
    "79,5422.726050005841\n",
    "80,4088.1381828730496\n",
    "81,4967.992270545817\n",
    "82,5015.697481475353\n",
    "83,5120.817700584382\n",
    "84,4315.799646068533\n",
    "85,4835.938888056308\n",
    "86,5893.133444219054\n",
    "87,4509.974195101194\n",
    "88,5154.981033043408\n",
    "89,4756.304894399503\n",
    "90,5035.578782220435\n",
    "91,4971.97205399659\n",
    "92,5264.229189323557\n",
    "93,5810.324473518437\n",
    "94,5195.027891079441\n",
    "95,5145.7653791843\n",
    "96,4897.259184556639\n",
    "97,5429.938909988875\n",
    "98,5443.355097107537\n",
    "99,4765.177934832559\n",
    "\n",
    "3. te\n",
    "0,0.1120665629226885\n",
    "1,0.1448484494235634\n",
    "2,0.12802071068611762\n",
    "3,0.12686616074333174\n",
    "4,0.11417722238927171\n",
    "5,0.14431783573811893\n",
    "6,0.14927099367427174\n",
    "7,0.11835518695834683\n",
    "8,0.104540652241308\n",
    "9,0.11813934608260267\n",
    "10,0.11991176406630617\n",
    "11,0.13935801491379995\n",
    "12,0.12374758565800831\n",
    "13,0.11918252980861\n",
    "14,0.12535937011174983\n",
    "15,0.13809263378907097\n",
    "16,0.13622218322144897\n",
    "17,0.11861752775220653\n",
    "18,0.12837964124466744\n",
    "19,0.13310270474790162\n",
    "20,0.11414667967166336\n",
    "21,0.15738185286949172\n",
    "22,0.13846191369281916\n",
    "23,0.1309763419731059\n",
    "24,0.11918574149788443\n",
    "25,0.10849208544440343\n",
    "26,0.12144241220933953\n",
    "27,0.12429918742904165\n",
    "28,0.12401341228278658\n",
    "29,0.11909251835848811\n",
    "30,0.12534313540457342\n",
    "31,0.13772528292676176\n",
    "32,0.14237334597556348\n",
    "33,0.1149908016804011\n",
    "34,0.11663034108936716\n",
    "35,0.13345979870563054\n",
    "36,0.13792373891235485\n",
    "37,0.12592205404551512\n",
    "38,0.11618716582806277\n",
    "39,0.1236946549074161\n",
    "40,0.1518154428950952\n",
    "41,0.11899933800978037\n",
    "42,0.1334104334260782\n",
    "43,0.12044933316350703\n",
    "44,0.15644423187185139\n",
    "45,0.11238427912355291\n",
    "46,0.1093926661253089\n",
    "47,0.1201130262486998\n",
    "48,0.12220893490988512\n",
    "49,0.12923868597048924\n",
    "50,0.11605447718880331\n",
    "51,0.12517486697614424\n",
    "52,0.134278481833615\n",
    "53,0.13259484396574414\n",
    "54,0.11609140521030992\n",
    "55,0.12742532655420236\n",
    "56,0.13499314819457234\n",
    "57,0.10859403980356808\n",
    "58,0.15071660643412657\n",
    "59,0.10887781628174753\n",
    "60,0.11741377662171941\n",
    "61,0.11678822582000394\n",
    "62,0.12946897402312088\n",
    "63,0.10815875501883998\n",
    "64,0.11519665336219592\n",
    "65,0.14072289953879782\n",
    "66,0.10680208387507033\n",
    "67,0.11679001282570174\n",
    "68,0.1458084399189047\n",
    "69,0.12329664410950791\n",
    "70,0.09896124419806995\n",
    "71,0.110764527136708\n",
    "72,0.11448840912744525\n",
    "73,0.1157901878695587\n",
    "74,0.12369770030845129\n",
    "75,0.10520201257422472\n",
    "76,0.11679178685757112\n",
    "77,0.12821726185318374\n",
    "78,0.10458019441962925\n",
    "79,0.14107570991744922\n",
    "80,0.1360445272398059\n",
    "81,0.12478398220148712\n",
    "82,0.13266027506933073\n",
    "83,0.10607844562993962\n",
    "84,0.10196898152963019\n",
    "85,0.14104560101320465\n",
    "86,0.1599296473332828\n",
    "87,0.13068769078279674\n",
    "88,0.11409966917269443\n",
    "89,0.13619639359241678\n",
    "90,0.12978284004793805\n",
    "91,0.13149475644232786\n",
    "92,0.14751679652468772\n",
    "93,0.1440229546456236\n",
    "94,0.14031287935662107\n",
    "95,0.13745168842971403\n",
    "96,0.12711315749420327\n",
    "97,0.13488508250140588\n",
    "98,0.12329059784840886\n",
    "99,0.11539854443483825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output, activation_fn=nn.ReLU, activation_scale=1.0):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = nn.Linear(n_hidden, n_output)   # output layer\n",
    "        self.activation_fn = activation_fn()           # instantiate the activation function\n",
    "        self.activation_scale = activation_scale       # scaling factor for the activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_scale * self.activation_fn(self.hidden(x))  # apply scaled activation function\n",
    "        x = self.predict(x)                                              # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "train_input=x_train.to_numpy()\n",
    "train_label=y_train.to_numpy()\n",
    "test_input=x_test.to_numpy()\n",
    "test_label=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler()\n",
    "\n",
    "train_input_scaled = scaler_input.fit_transform(train_input)\n",
    "test_input_scaled = scaler_input.transform(test_input)\n",
    "\n",
    "dataset['train_input'] = torch.from_numpy(train_input_scaled).type(dtype)\n",
    "dataset['test_input'] = torch.from_numpy(test_input_scaled).type(dtype)\n",
    "dataset['train_label'] = torch.from_numpy(train_label[:,None]).type(dtype)\n",
    "dataset['test_label'] = torch.from_numpy(test_label[:,None]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-1)),\n",
    "    'lambda_l1': hp.loguniform('lambda_l1', np.log(1e-7), np.log(1e-1)),\n",
    "    'lambda_l2': hp.loguniform('lambda_l2', np.log(1e-7), np.log(1e-1)),\n",
    "    'non': hp.quniform('non', 8, 20, 2),\n",
    "    'activation_scale': hp.uniform('activation_scale', 0.1, 10.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(columns=['params', 'loss_curve_train', 'loss_curve_test', 'best_score'])\n",
    "trial_no = 0\n",
    "bestest_score = float('inf')  # Initialize global variable\n",
    "\n",
    "set_seed(42)\n",
    "def objective(params):\n",
    "    set_seed(42)\n",
    "    global trial_no,bestest_score\n",
    "    trial_no += 1\n",
    "    params['non'] = int(params['non'])\n",
    "    model = Net(\n",
    "        n_feature=8,\n",
    "        n_hidden=params['non'],\n",
    "        n_output=1,\n",
    "        activation_fn=nn.SiLU,\n",
    "        activation_scale=params['activation_scale']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['lambda_l2'])\n",
    "    num_epochs = 5000\n",
    "    patience = 200\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    param_df['params'].loc[trial_no] = params\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(dataset['train_input'])\n",
    "        main_loss = criterion(predictions, dataset['train_label'])\n",
    "        l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = main_loss + (params['lambda_l1'] * l1_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(dataset['test_input'])\n",
    "            temp_score = mse(dataset['test_label'].numpy(), val_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "        test_loss.append(temp_score)\n",
    "        \n",
    "        \n",
    "        if temp_score < best_loss:\n",
    "            best_loss = temp_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        # Update the bestest score\n",
    "        if temp_score < bestest_score:\n",
    "            bestest_score = temp_score\n",
    "            torch.save({'model_state_dict': best_model_state, 'params': params}, f\"{current_target}_bestest_model.pth\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(dataset['test_input'])\n",
    "    score = mse(dataset['test_label'].numpy(), test_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "    param_df.loc[trial_no] = [params, training_loss, test_loss, score]\n",
    "    print(score)\n",
    "    print(params)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4548944230698115                                    \n",
      "{'activation_scale': 4.219211513840314, 'lambda_l1': 2.4123831736930352e-05, 'lambda_l2': 0.0029370196727864463, 'learning_rate': 0.061234442942182006, 'non': 14}\n",
      "408.4069961626172                                                               \n",
      "{'activation_scale': 7.991691919952546, 'lambda_l1': 0.028001073970091335, 'lambda_l2': 0.0016130722511035914, 'learning_rate': 7.938509437337308e-05, 'non': 14}\n",
      "503.5010137909091                                                               \n",
      "{'activation_scale': 8.622899930605518, 'lambda_l1': 1.1331171896840189e-07, 'lambda_l2': 0.00035466355498974856, 'learning_rate': 2.1449943391555776e-05, 'non': 14}\n",
      "489.6492998058572                                                               \n",
      "{'activation_scale': 3.7186399967483426, 'lambda_l1': 1.3490803986353162e-06, 'lambda_l2': 0.0030039923783754655, 'learning_rate': 5.98137420848197e-05, 'non': 10}\n",
      "25.28706703204448                                                               \n",
      "{'activation_scale': 6.9547988405915895, 'lambda_l1': 0.0001196748834591767, 'lambda_l2': 8.423938288890651e-07, 'learning_rate': 0.001417889934777308, 'non': 12}\n",
      "107.39566634590042                                                              \n",
      "{'activation_scale': 4.512064096064058, 'lambda_l1': 2.2712597281950124e-05, 'lambda_l2': 3.035487740815411e-06, 'learning_rate': 0.00025571076678157224, 'non': 14}\n",
      "491.516461750058                                                                \n",
      "{'activation_scale': 8.936037719625505, 'lambda_l1': 0.01160877823918794, 'lambda_l2': 2.4680307992420038e-05, 'learning_rate': 2.877057997246275e-05, 'non': 20}\n",
      "96.5035294758753                                                                \n",
      "{'activation_scale': 9.221817948664254, 'lambda_l1': 0.001940916000094957, 'lambda_l2': 0.00022596176915516932, 'learning_rate': 0.0001681154179640637, 'non': 20}\n",
      "2.8798092590357918                                                              \n",
      "{'activation_scale': 8.042073659033116, 'lambda_l1': 0.0025246523650217104, 'lambda_l2': 9.016395916531196e-06, 'learning_rate': 0.08523987378178254, 'non': 16}\n",
      "3.982345971886521                                                               \n",
      "{'activation_scale': 9.17349530055881, 'lambda_l1': 3.3356414566097455e-06, 'lambda_l2': 2.1461941727437876e-06, 'learning_rate': 0.003640513044898941, 'non': 18}\n",
      "2.6846359974305627                                                               \n",
      "{'activation_scale': 9.670415187075179, 'lambda_l1': 0.053373620501912, 'lambda_l2': 3.3845414132859576e-06, 'learning_rate': 0.022769995960281952, 'non': 18}\n",
      "3.4119049677422373                                                               \n",
      "{'activation_scale': 8.365443629563499, 'lambda_l1': 3.295853437384608e-05, 'lambda_l2': 3.401268373317416e-05, 'learning_rate': 0.007859576608388971, 'non': 12}\n",
      "507.60725600416464                                                               \n",
      "{'activation_scale': 6.128322326212187, 'lambda_l1': 7.858755815801225e-07, 'lambda_l2': 2.4602732232106245e-07, 'learning_rate': 1.7594298125370264e-05, 'non': 14}\n",
      "444.35233158648055                                                               \n",
      "{'activation_scale': 7.401536421393148, 'lambda_l1': 1.0539622952500739e-06, 'lambda_l2': 6.618120470441602e-06, 'learning_rate': 5.9625627003298015e-05, 'non': 18}\n",
      "428.087361175609                                                                 \n",
      "{'activation_scale': 0.27878316677894793, 'lambda_l1': 5.5285452286948796e-05, 'lambda_l2': 4.357238561283265e-05, 'learning_rate': 0.0003575716885380909, 'non': 18}\n",
      "7.549858127331866                                                                \n",
      "{'activation_scale': 7.287932100122851, 'lambda_l1': 4.71374323967087e-05, 'lambda_l2': 4.8686101821240676e-05, 'learning_rate': 0.002131425197528528, 'non': 18}\n",
      "3.7582695594758855                                                               \n",
      "{'activation_scale': 8.057450293070128, 'lambda_l1': 0.0003035005355112547, 'lambda_l2': 0.0002735684538159516, 'learning_rate': 0.005367009240783987, 'non': 12}\n",
      "228.41646471898758                                                               \n",
      "{'activation_scale': 9.58207875542462, 'lambda_l1': 3.46828754801677e-05, 'lambda_l2': 1.5462874608527336e-06, 'learning_rate': 0.00012753982287270883, 'non': 14}\n",
      "2.556904051153821                                                                \n",
      "{'activation_scale': 5.629665542380779, 'lambda_l1': 0.003054368563330457, 'lambda_l2': 2.102494814680586e-07, 'learning_rate': 0.09177881002074842, 'non': 14}\n",
      "3.465086445695042                                                                \n",
      "{'activation_scale': 7.3776203014932555, 'lambda_l1': 3.469005335498986e-05, 'lambda_l2': 2.5388571812730076e-06, 'learning_rate': 0.007260757384534137, 'non': 18}\n",
      "2.9641155421185355                                                               \n",
      "{'activation_scale': 2.6430959030312153, 'lambda_l1': 0.000747910637828343, 'lambda_l2': 0.035710881328816814, 'learning_rate': 0.08966275237188831, 'non': 8}\n",
      "3.1157654116157385                                                               \n",
      "{'activation_scale': 1.9469186968500956, 'lambda_l1': 6.8805750116819216e-06, 'lambda_l2': 0.06222635383468529, 'learning_rate': 0.03092803396319775, 'non': 16}\n",
      "2.6385061276337622                                                               \n",
      "{'activation_scale': 5.672652596573295, 'lambda_l1': 0.003557112322268052, 'lambda_l2': 0.010885928369310141, 'learning_rate': 0.031768545065074746, 'non': 16}\n",
      "2.821286333498118                                                                \n",
      "{'activation_scale': 4.232637839464612, 'lambda_l1': 0.00021862388833060264, 'lambda_l2': 0.0022716692629179214, 'learning_rate': 0.015576921377596302, 'non': 10}\n",
      "2.8628119928338602                                                               \n",
      "{'activation_scale': 3.145423925691292, 'lambda_l1': 1.8977001860691078e-07, 'lambda_l2': 0.010744556830956023, 'learning_rate': 0.05898233431682437, 'non': 12}\n",
      "64.51460415664599                                                                \n",
      "{'activation_scale': 5.360098687298999, 'lambda_l1': 0.0005794439101671576, 'lambda_l2': 1.0288963180940992e-07, 'learning_rate': 0.0005887448004946864, 'non': 10}\n",
      "2.5911071455034556                                                               \n",
      "{'activation_scale': 2.103111370599446, 'lambda_l1': 0.008313325186177965, 'lambda_l2': 0.0008965312828146251, 'learning_rate': 0.05328080079017865, 'non': 16}\n",
      "3.963521655940658                                                                \n",
      "{'activation_scale': 0.8975748438783286, 'lambda_l1': 0.08764525227805703, 'lambda_l2': 0.010999877349367828, 'learning_rate': 0.013954505173739301, 'non': 12}\n",
      "2.255285700299794                                                                \n",
      "{'activation_scale': 6.4067262673132275, 'lambda_l1': 1.003039606374122e-05, 'lambda_l2': 0.0007338342441566595, 'learning_rate': 0.09998026002570609, 'non': 16}\n",
      "2.350092688283483                                                                \n",
      "{'activation_scale': 6.573904658099618, 'lambda_l1': 7.835297697435863e-06, 'lambda_l2': 0.0008470068291862547, 'learning_rate': 0.04379096827592362, 'non': 16}\n",
      "40.16500290865986                                                               \n",
      "{'activation_scale': 6.455475689183339, 'lambda_l1': 9.35021290350759e-06, 'lambda_l2': 0.0005514904943834626, 'learning_rate': 0.000838965415136643, 'non': 16}\n",
      "5.7920597425484885                                                              \n",
      "{'activation_scale': 4.826586343401416, 'lambda_l1': 4.183715559498389e-07, 'lambda_l2': 8.997885417884049e-05, 'learning_rate': 0.0029284857216477864, 'non': 20}\n",
      "2.5137634229894172                                                              \n",
      "{'activation_scale': 6.709426560732815, 'lambda_l1': 3.1444284843188417e-06, 'lambda_l2': 0.004416223824195425, 'learning_rate': 0.04264552309913075, 'non': 16}\n",
      "2.8121565291561352                                                              \n",
      "{'activation_scale': 5.943396837976361, 'lambda_l1': 1.0375221102305872e-05, 'lambda_l2': 0.0011061674264617248, 'learning_rate': 0.015622600790637093, 'non': 16}\n",
      "19.628471733032175                                                              \n",
      "{'activation_scale': 3.4093044664114727, 'lambda_l1': 2.6650706196241044e-06, 'lambda_l2': 0.00012717521695296394, 'learning_rate': 0.001924936633777395, 'non': 20}\n",
      "3.2159715572012617                                                              \n",
      "{'activation_scale': 4.86516634581224, 'lambda_l1': 2.0403771924116339e-07, 'lambda_l2': 0.005225389146700399, 'learning_rate': 0.011178662489075356, 'non': 14}\n",
      "2.7103711995695408                                                              \n",
      "{'activation_scale': 3.8339957775187177, 'lambda_l1': 0.00010074665229776366, 'lambda_l2': 0.0893706826078776, 'learning_rate': 0.09742079100014223, 'non': 16}\n",
      "24.78219262810896                                                               \n",
      "{'activation_scale': 6.39001170549723, 'lambda_l1': 1.6724976204391697e-05, 'lambda_l2': 1.440832316245216e-05, 'learning_rate': 0.0011627147780238884, 'non': 20}\n",
      "2.631011817629761                                                               \n",
      "{'activation_scale': 7.752652817334932, 'lambda_l1': 5.15151622608675e-07, 'lambda_l2': 0.022622569784412027, 'learning_rate': 0.02701343477948079, 'non': 18}\n",
      "2.429071024196278                                                               \n",
      "{'activation_scale': 6.9743644779442535, 'lambda_l1': 1.845805177069469e-06, 'lambda_l2': 0.0015450694560524185, 'learning_rate': 0.04626681764673966, 'non': 14}\n",
      "508.52184623887166                                                              \n",
      "{'activation_scale': 8.664330872592984, 'lambda_l1': 5.54734492325098e-06, 'lambda_l2': 0.00014878993653502144, 'learning_rate': 1.0537407863140354e-05, 'non': 16}\n",
      "2.7262922691362905                                                              \n",
      "{'activation_scale': 5.250934286446425, 'lambda_l1': 1.8964509667364095e-05, 'lambda_l2': 7.342761546174491e-05, 'learning_rate': 0.019283587795048563, 'non': 10}\n",
      "4.121743039952095                                                               \n",
      "{'activation_scale': 4.264727444380104, 'lambda_l1': 1.4648249325893185e-07, 'lambda_l2': 0.0006776701136998206, 'learning_rate': 0.004405039507548967, 'non': 18}\n",
      "3.335696384917438                                                               \n",
      "{'activation_scale': 9.854004015712672, 'lambda_l1': 3.413421783962141e-07, 'lambda_l2': 0.0004737288269624489, 'learning_rate': 0.007607067272997731, 'non': 14}\n",
      "483.55084689895796                                                              \n",
      "{'activation_scale': 8.465892201973242, 'lambda_l1': 0.00015171349371177086, 'lambda_l2': 0.00024733161328996075, 'learning_rate': 3.5255588952567705e-05, 'non': 18}\n",
      "67.20324039363403                                                               \n",
      "{'activation_scale': 7.849650649460598, 'lambda_l1': 1.5849361968169974e-06, 'lambda_l2': 2.2127929098600515e-05, 'learning_rate': 0.00034374662105613667, 'non': 20}\n",
      "2.34660180406209                                                                \n",
      "{'activation_scale': 9.111293117591906, 'lambda_l1': 0.0003736889192233152, 'lambda_l2': 0.004005967390333229, 'learning_rate': 0.07253956466477611, 'non': 12}\n",
      "2.450999110028546                                                               \n",
      "{'activation_scale': 9.294260558546627, 'lambda_l1': 0.013137535214109281, 'lambda_l2': 0.004321413678277541, 'learning_rate': 0.07301936265960134, 'non': 12}\n",
      "3.1325611947974226                                                              \n",
      "{'activation_scale': 8.936672419743125, 'lambda_l1': 0.0007886954448529726, 'lambda_l2': 0.024520092946662676, 'learning_rate': 0.00927673585840077, 'non': 10}\n",
      "419.2440427373035                                                               \n",
      "{'activation_scale': 6.9274596195347105, 'lambda_l1': 6.216411119917549e-05, 'lambda_l2': 5.7160077597299026e-06, 'learning_rate': 0.00012743813141444962, 'non': 8}\n",
      "100%|██████████| 50/50 [20:50<00:00, 25.02s/trial, best loss: 2.255285700299794]\n",
      "Best hyperparameters: {'activation_scale': 6.4067262673132275, 'lambda_l1': 1.003039606374122e-05, 'lambda_l2': 0.0007338342441566595, 'learning_rate': 0.09998026002570609, 'non': 16.0}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=tpe.suggest,\n",
    "                   max_evals=50,\n",
    "                   trials=trials)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df.to_csv(f'param_df{current_target}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_7888\\90643497.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n",
    "params1 = checkpoint['params']\n",
    "model = Net(\n",
    "    n_feature=8,\n",
    "    n_hidden=params1['non'],\n",
    "    n_output=1,\n",
    "    activation_fn=nn.SiLU,\n",
    "    activation_scale=params1['activation_scale']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.255285700299794"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(dataset['test_input'])\n",
    "mse(dataset['test_label'].numpy(), test_predictions.numpy())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999383242909991"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(dataset['test_label'].numpy(), test_predictions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv',header=1)\n",
    "df=df.drop(columns=['power','te'])\n",
    "current_target='thr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "x = df.drop(columns=current_target)\n",
    "y = df[current_target]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=80) #59 power, 80 thr, 70 te\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "train_input=x_train.to_numpy()\n",
    "train_label=y_train.to_numpy()\n",
    "test_input=x_test.to_numpy()\n",
    "test_label=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler()\n",
    "\n",
    "train_input_scaled = scaler_input.fit_transform(train_input)\n",
    "test_input_scaled = scaler_input.transform(test_input)\n",
    "\n",
    "dataset['train_input'] = torch.from_numpy(train_input_scaled).type(dtype)\n",
    "dataset['test_input'] = torch.from_numpy(test_input_scaled).type(dtype)\n",
    "dataset['train_label'] = torch.from_numpy(train_label[:,None]).type(dtype)\n",
    "dataset['test_label'] = torch.from_numpy(test_label[:,None]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-1)),\n",
    "    'lambda_l1': hp.loguniform('lambda_l1', np.log(1e-7), np.log(1e-1)),\n",
    "    'lambda_l2': hp.loguniform('lambda_l2', np.log(1e-7), np.log(1e-1)),\n",
    "    'non': hp.quniform('non', 8, 20, 2),\n",
    "    'activation_scale': hp.uniform('activation_scale', 0.1, 10.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(columns=['params', 'loss_curve_train', 'loss_curve_test', 'best_score'])\n",
    "trial_no = 0\n",
    "bestest_score = float('inf')  # Initialize global variable\n",
    "\n",
    "set_seed(42)\n",
    "def objective(params):\n",
    "    set_seed(42)\n",
    "    global trial_no,bestest_score\n",
    "    trial_no += 1\n",
    "    params['non'] = int(params['non'])\n",
    "    model = Net(\n",
    "        n_feature=8,\n",
    "        n_hidden=params['non'],\n",
    "        n_output=1,\n",
    "        activation_fn=nn.SiLU,\n",
    "        activation_scale=params['activation_scale']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['lambda_l2'])\n",
    "    num_epochs = 5000\n",
    "    patience = 200\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    param_df['params'].loc[trial_no] = params\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(dataset['train_input'])\n",
    "        main_loss = criterion(predictions, dataset['train_label'])\n",
    "        l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = main_loss + (params['lambda_l1'] * l1_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(dataset['test_input'])\n",
    "            temp_score = mse(dataset['test_label'].numpy(), val_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "        test_loss.append(temp_score)\n",
    "        \n",
    "        \n",
    "        if temp_score < best_loss:\n",
    "            best_loss = temp_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        # Update the bestest score\n",
    "        if temp_score < bestest_score:\n",
    "            bestest_score = temp_score\n",
    "            torch.save({'model_state_dict': best_model_state, 'params': params}, f\"{current_target}_bestest_model.pth\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(dataset['test_input'])\n",
    "    score = mse(dataset['test_label'].numpy(), test_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "    param_df.loc[trial_no] = [params, training_loss, test_loss, score]\n",
    "    print(score)\n",
    "    print(params)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203.4303705168914                                     \n",
      "{'activation_scale': 4.885348584562357, 'lambda_l1': 0.0013056196102088614, 'lambda_l2': 3.2394824495907965e-07, 'learning_rate': 0.006932173493346886, 'non': 8}\n",
      "6166.899984210655                                                              \n",
      "{'activation_scale': 4.500807718416699, 'lambda_l1': 6.861980960685561e-05, 'lambda_l2': 4.1268314072025697e-07, 'learning_rate': 0.00042451893965459575, 'non': 20}\n",
      "1155.2693760025038                                                             \n",
      "{'activation_scale': 0.8940034800852334, 'lambda_l1': 0.06910061361500933, 'lambda_l2': 3.167390798914673e-05, 'learning_rate': 0.0045708478619058196, 'non': 18}\n",
      "7869.436142091858                                                              \n",
      "{'activation_scale': 4.723222711485381, 'lambda_l1': 0.005312942757796994, 'lambda_l2': 0.009824842211797075, 'learning_rate': 7.619700504087682e-05, 'non': 8}\n",
      "141.6457722949283                                                              \n",
      "{'activation_scale': 7.210703491579844, 'lambda_l1': 9.088338422531117e-05, 'lambda_l2': 0.0001303414751841282, 'learning_rate': 0.005020215441011286, 'non': 10}\n",
      "7857.130651350561                                                              \n",
      "{'activation_scale': 0.5357681611120637, 'lambda_l1': 0.0014313865549668377, 'lambda_l2': 8.877622541230228e-06, 'learning_rate': 0.0002013150898289602, 'non': 10}\n",
      "63.335823605990754                                                             \n",
      "{'activation_scale': 2.6773654689461637, 'lambda_l1': 1.7809987848931113e-06, 'lambda_l2': 0.001169692127870201, 'learning_rate': 0.053073899875986666, 'non': 12}\n",
      "7845.827406594337                                                               \n",
      "{'activation_scale': 1.3719876800631194, 'lambda_l1': 0.0347656004656673, 'lambda_l2': 9.213147714158601e-05, 'learning_rate': 0.00011679253190120838, 'non': 18}\n",
      "7891.362256380284                                                               \n",
      "{'activation_scale': 1.1906561586656175, 'lambda_l1': 8.986590619645828e-05, 'lambda_l2': 0.000199337467078624, 'learning_rate': 2.028937894957498e-05, 'non': 16}\n",
      "7889.344323280771                                                               \n",
      "{'activation_scale': 2.490959614690722, 'lambda_l1': 0.015912478396893234, 'lambda_l2': 0.00012723347249072046, 'learning_rate': 2.0229513749270775e-05, 'non': 18}\n",
      "2773.858897751747                                                                \n",
      "{'activation_scale': 5.717256184804286, 'lambda_l1': 7.837578211536958e-07, 'lambda_l2': 7.341418632551651e-05, 'learning_rate': 0.001015294542156441, 'non': 12}\n",
      "6668.591825028733                                                                \n",
      "{'activation_scale': 4.65931171617706, 'lambda_l1': 4.249855365854886e-05, 'lambda_l2': 1.942324021851508e-06, 'learning_rate': 0.0003687126432693734, 'non': 14}\n",
      "6521.905846512111                                                                \n",
      "{'activation_scale': 1.45109767362074, 'lambda_l1': 0.011503085655402418, 'lambda_l2': 7.25674426631281e-05, 'learning_rate': 0.0007540865098643605, 'non': 16}\n",
      "70.53736721078312                                                                \n",
      "{'activation_scale': 0.5118249299001738, 'lambda_l1': 0.015161046257533199, 'lambda_l2': 0.053147390347084905, 'learning_rate': 0.05477644555102591, 'non': 14}\n",
      "7890.260033503058                                                                \n",
      "{'activation_scale': 2.5991813106456485, 'lambda_l1': 0.0003460222968335972, 'lambda_l2': 0.007202798629959226, 'learning_rate': 1.617342082877214e-05, 'non': 12}\n",
      "1093.883454631279                                                                \n",
      "{'activation_scale': 2.15042486073933, 'lambda_l1': 0.0004148442022599809, 'lambda_l2': 0.013345785602477336, 'learning_rate': 0.0030335085973073163, 'non': 14}\n",
      "156.85375852443622                                                               \n",
      "{'activation_scale': 9.908516304245651, 'lambda_l1': 4.257187829310088e-07, 'lambda_l2': 0.00017114586126241953, 'learning_rate': 0.006725797036308686, 'non': 20}\n",
      "7802.419738255163                                                                \n",
      "{'activation_scale': 3.0134696241807033, 'lambda_l1': 4.0598588512691745e-05, 'lambda_l2': 0.00012897750971922598, 'learning_rate': 0.00012424144895416717, 'non': 16}\n",
      "67.71205069720243                                                                \n",
      "{'activation_scale': 9.299761452919915, 'lambda_l1': 0.05888320469832125, 'lambda_l2': 0.0015378608624586532, 'learning_rate': 0.01675159482336115, 'non': 12}\n",
      "4266.049239301234                                                                \n",
      "{'activation_scale': 7.46025215297969, 'lambda_l1': 0.0008225870585884039, 'lambda_l2': 9.758530022924904e-05, 'learning_rate': 0.0006288658042669955, 'non': 10}\n",
      "61.6157761971548                                                                 \n",
      "{'activation_scale': 8.800467566987368, 'lambda_l1': 2.5683409721438868e-06, 'lambda_l2': 0.0016679211044937007, 'learning_rate': 0.08203389122818412, 'non': 12}\n",
      "62.62569477822985                                                                \n",
      "{'activation_scale': 3.6308253302895146, 'lambda_l1': 3.084618858521874e-06, 'lambda_l2': 0.001245969319326975, 'learning_rate': 0.05246587792093819, 'non': 12}\n",
      "65.23855685971789                                                              \n",
      "{'activation_scale': 8.077398055770074, 'lambda_l1': 6.517229538298419e-06, 'lambda_l2': 0.0009571725687593616, 'learning_rate': 0.023836208883478323, 'non': 10}\n",
      "70.54466309189364                                                              \n",
      "{'activation_scale': 6.183293249726654, 'lambda_l1': 1.252379951105372e-07, 'lambda_l2': 0.071597949103186, 'learning_rate': 0.098230048528413, 'non': 12}\n",
      "76.76511833888954                                                              \n",
      "{'activation_scale': 3.4960602508387346, 'lambda_l1': 7.125660232663925e-06, 'lambda_l2': 0.0027656479148605376, 'learning_rate': 0.020762242705828913, 'non': 8}\n",
      "71.18065800992804                                                              \n",
      "{'activation_scale': 8.603084352345249, 'lambda_l1': 1.0457483433051498e-07, 'lambda_l2': 0.0005074134213833408, 'learning_rate': 0.09263897337041194, 'non': 14}\n",
      "1898.1950489033825                                                             \n",
      "{'activation_scale': 3.802612487212079, 'lambda_l1': 1.0134108499234216e-05, 'lambda_l2': 0.025514884315880625, 'learning_rate': 0.0020191162840533937, 'non': 10}\n",
      "63.87696693960936                                                              \n",
      "{'activation_scale': 5.958828386713632, 'lambda_l1': 2.828645174253162e-06, 'lambda_l2': 0.0038773409274997974, 'learning_rate': 0.04543022278243747, 'non': 12}\n",
      "87.64577907542751                                                              \n",
      "{'activation_scale': 6.503000060988283, 'lambda_l1': 3.481384161891512e-07, 'lambda_l2': 1.7210299920908007e-05, 'learning_rate': 0.011825706632585222, 'non': 8}\n",
      "96.4954257157507                                                               \n",
      "{'activation_scale': 3.94968534186998, 'lambda_l1': 1.9640325159461966e-05, 'lambda_l2': 0.0004786352602564694, 'learning_rate': 0.009397228508392788, 'non': 10}\n",
      "63.757281289320424                                                             \n",
      "{'activation_scale': 5.389733357158783, 'lambda_l1': 2.042420219979598e-06, 'lambda_l2': 1.4716686228740446e-07, 'learning_rate': 0.03107682599574154, 'non': 14}\n",
      "61.97107239604614                                                              \n",
      "{'activation_scale': 7.101016729503279, 'lambda_l1': 2.619414278920197e-07, 'lambda_l2': 3.181993366357411e-06, 'learning_rate': 0.09993677479352818, 'non': 16}\n",
      "63.144526296634965                                                             \n",
      "{'activation_scale': 9.074377366241, 'lambda_l1': 2.54983549276978e-07, 'lambda_l2': 1.1843423857717776e-06, 'learning_rate': 0.08802605851513101, 'non': 16}\n",
      "737.052288198675                                                               \n",
      "{'activation_scale': 6.853709939317361, 'lambda_l1': 1.0891761385824395e-06, 'lambda_l2': 3.8393857724043605e-06, 'learning_rate': 0.0019545874958641946, 'non': 20}\n",
      "72.75476079029623                                                              \n",
      "{'activation_scale': 9.977533620021152, 'lambda_l1': 1.0351270443871445e-07, 'lambda_l2': 9.485343749355373e-07, 'learning_rate': 0.011870896653862019, 'non': 18}\n",
      "7875.315751973936                                                              \n",
      "{'activation_scale': 7.808406128494188, 'lambda_l1': 6.303683100757186e-07, 'lambda_l2': 3.124080698468468e-07, 'learning_rate': 3.253630561320203e-05, 'non': 16}\n",
      "159.3913985468631                                                              \n",
      "{'activation_scale': 8.18270147960132, 'lambda_l1': 1.8647729574407455e-07, 'lambda_l2': 2.382394020227796e-05, 'learning_rate': 0.0036012186790246, 'non': 18}\n",
      "62.62275605250291                                                              \n",
      "{'activation_scale': 9.282157975049532, 'lambda_l1': 0.00018571560147374988, 'lambda_l2': 5.195199928395092e-06, 'learning_rate': 0.029234914989069855, 'non': 16}\n",
      "7889.202681792584                                                              \n",
      "{'activation_scale': 7.073071821745653, 'lambda_l1': 1.3911300692558578e-05, 'lambda_l2': 9.322972695217305e-06, 'learning_rate': 1.009737632008498e-05, 'non': 20}\n",
      "63.61204224571806                                                              \n",
      "{'activation_scale': 8.653760586449796, 'lambda_l1': 0.002407997623429028, 'lambda_l2': 1.1557879137925993e-07, 'learning_rate': 0.07455954824290241, 'non': 18}\n",
      "62.811025820383406                                                             \n",
      "{'activation_scale': 5.037894358193731, 'lambda_l1': 1.2297110380370076e-06, 'lambda_l2': 4.0523220641343137e-07, 'learning_rate': 0.036434202086030806, 'non': 14}\n",
      "7416.148159022141                                                              \n",
      "{'activation_scale': 7.49590010661333, 'lambda_l1': 2.647630600839395e-05, 'lambda_l2': 0.0003545122364176075, 'learning_rate': 0.0002872984607791047, 'non': 8}\n",
      "112.68268090335532                                                             \n",
      "{'activation_scale': 6.62423759947494, 'lambda_l1': 4.996806954374132e-06, 'lambda_l2': 4.12524093752415e-05, 'learning_rate': 0.005765250745954014, 'non': 16}\n",
      "1676.050827200644                                                              \n",
      "{'activation_scale': 9.787552963392557, 'lambda_l1': 6.20545924940629e-07, 'lambda_l2': 1.2641920867952557e-05, 'learning_rate': 0.0017130159185270713, 'non': 14}\n",
      "69.09937345035563                                                              \n",
      "{'activation_scale': 4.413115899103022, 'lambda_l1': 0.00014355664374027236, 'lambda_l2': 2.848583921879853e-06, 'learning_rate': 0.014701847698507221, 'non': 16}\n",
      "7851.3634518645                                                                \n",
      "{'activation_scale': 8.589633434525592, 'lambda_l1': 3.6490202723834293e-06, 'lambda_l2': 0.005475448091233386, 'learning_rate': 4.339711093335039e-05, 'non': 18}\n",
      "62.377625041453385                                                             \n",
      "{'activation_scale': 5.507749550403608, 'lambda_l1': 2.4350110314980795e-07, 'lambda_l2': 0.022123003934319162, 'learning_rate': 0.06516788910871137, 'non': 14}\n",
      "2643.1230574123024                                                             \n",
      "{'activation_scale': 6.221647051866901, 'lambda_l1': 6.962363828528431e-05, 'lambda_l2': 0.0002695443247754712, 'learning_rate': 0.0010306866033998848, 'non': 12}\n",
      "81.143918965837                                                                \n",
      "{'activation_scale': 7.793560871178609, 'lambda_l1': 1.5887731495801903e-06, 'lambda_l2': 3.4862451376674914e-05, 'learning_rate': 0.008798520426081851, 'non': 10}\n",
      "147.30068988912936                                                             \n",
      "{'activation_scale': 8.918343035669094, 'lambda_l1': 3.864683214772773e-05, 'lambda_l2': 5.2378674230975255e-05, 'learning_rate': 0.004131200137837249, 'non': 20}\n",
      "100%|██████████| 50/50 [20:12<00:00, 24.24s/trial, best loss: 61.6157761971548]\n",
      "Best hyperparameters: {'activation_scale': 8.800467566987368, 'lambda_l1': 2.5683409721438868e-06, 'lambda_l2': 0.0016679211044937007, 'learning_rate': 0.08203389122818412, 'non': 12.0}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=tpe.suggest,\n",
    "                   max_evals=50,\n",
    "                   trials=trials)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df.to_csv(f'param_df{current_target}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_7888\\90643497.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n",
    "params1 = checkpoint['params']\n",
    "model = Net(\n",
    "    n_feature=8,\n",
    "    n_hidden=params1['non'],\n",
    "    n_output=1,\n",
    "    activation_fn=nn.SiLU,\n",
    "    activation_scale=params1['activation_scale']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.18275823895748"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(dataset['test_input'])\n",
    "mse(dataset['test_label'].numpy(), test_predictions.numpy())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8645112233303498"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(dataset['test_label'].numpy(), test_predictions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv',header=1)\n",
    "df=df.drop(columns=['power','thr'])\n",
    "current_target='te'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "x = df.drop(columns=current_target)\n",
    "y = df[current_target]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=70) #59 power, 80 thr, 70 te\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "train_input=x_train.to_numpy()\n",
    "train_label=y_train.to_numpy()\n",
    "test_input=x_test.to_numpy()\n",
    "test_label=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler()\n",
    "\n",
    "train_input_scaled = scaler_input.fit_transform(train_input)\n",
    "test_input_scaled = scaler_input.transform(test_input)\n",
    "\n",
    "dataset['train_input'] = torch.from_numpy(train_input_scaled).type(dtype)\n",
    "dataset['test_input'] = torch.from_numpy(test_input_scaled).type(dtype)\n",
    "dataset['train_label'] = torch.from_numpy(train_label[:,None]).type(dtype)\n",
    "dataset['test_label'] = torch.from_numpy(test_label[:,None]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-1)),\n",
    "    'lambda_l1': hp.loguniform('lambda_l1', np.log(1e-7), np.log(1e-1)),\n",
    "    'lambda_l2': hp.loguniform('lambda_l2', np.log(1e-7), np.log(1e-1)),\n",
    "    'non': hp.quniform('non', 8, 20, 2),\n",
    "    'activation_scale': hp.uniform('activation_scale', 0.1, 10.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(columns=['params', 'loss_curve_train', 'loss_curve_test', 'best_score'])\n",
    "trial_no = 0\n",
    "bestest_score = float('inf')  # Initialize global variable\n",
    "\n",
    "set_seed(42)\n",
    "def objective(params):\n",
    "    set_seed(42)\n",
    "    global trial_no,bestest_score\n",
    "    trial_no += 1\n",
    "    params['non'] = int(params['non'])\n",
    "    model = Net(\n",
    "        n_feature=8,\n",
    "        n_hidden=params['non'],\n",
    "        n_output=1,\n",
    "        activation_fn=nn.SiLU,\n",
    "        activation_scale=params['activation_scale']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['lambda_l2'])\n",
    "    num_epochs = 5000\n",
    "    patience = 200\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    param_df['params'].loc[trial_no] = params\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(dataset['train_input'])\n",
    "        main_loss = criterion(predictions, dataset['train_label'])\n",
    "        l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = main_loss + (params['lambda_l1'] * l1_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(dataset['test_input'])\n",
    "            temp_score = mse(dataset['test_label'].numpy(), val_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "        test_loss.append(temp_score)\n",
    "        \n",
    "        \n",
    "        if temp_score < best_loss:\n",
    "            best_loss = temp_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        # Update the bestest score\n",
    "        if temp_score < bestest_score:\n",
    "            bestest_score = temp_score\n",
    "            torch.save({'model_state_dict': best_model_state, 'params': params}, f\"{current_target}_bestest_model.pth\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(dataset['test_input'])\n",
    "    score = mse(dataset['test_label'].numpy(), test_predictions.numpy())**0.5  # Move tensors to CPU before numpy\n",
    "    param_df.loc[trial_no] = [params, training_loss, test_loss, score]\n",
    "    print(score)\n",
    "    print(params)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39507655002785863                                   \n",
      "{'activation_scale': 8.893226698398712, 'lambda_l1': 2.721305241375482e-07, 'lambda_l2': 0.0009786164460108963, 'learning_rate': 0.0031734184461203723, 'non': 12}\n",
      "0.38661945065716585                                                              \n",
      "{'activation_scale': 4.747423274055419, 'lambda_l1': 0.00363843003413488, 'lambda_l2': 4.3061020716509074e-05, 'learning_rate': 0.036310142240231144, 'non': 10}\n",
      "7.5851961709898                                                                  \n",
      "{'activation_scale': 7.41786030247314, 'lambda_l1': 0.001396322887209748, 'lambda_l2': 2.224633830352677e-06, 'learning_rate': 0.00011091409103511326, 'non': 18}\n",
      "1.1613466480250731                                                               \n",
      "{'activation_scale': 3.528166732846872, 'lambda_l1': 8.54000674858991e-06, 'lambda_l2': 0.00035874152375916076, 'learning_rate': 0.0006908815590392044, 'non': 16}\n",
      "0.39517646744922                                                                 \n",
      "{'activation_scale': 4.955345153780548, 'lambda_l1': 4.656867082841489e-05, 'lambda_l2': 0.016924013707451363, 'learning_rate': 0.0022216259237714214, 'non': 10}\n",
      "0.37024640326155855                                                              \n",
      "{'activation_scale': 8.640122732369452, 'lambda_l1': 2.7392136418879235e-07, 'lambda_l2': 3.6325264701148973e-07, 'learning_rate': 0.053617419872259035, 'non': 12}\n",
      "0.38070673532694527                                                              \n",
      "{'activation_scale': 1.4615132444025232, 'lambda_l1': 0.0021979250849230663, 'lambda_l2': 1.1395341750476912e-06, 'learning_rate': 0.03888284235776018, 'non': 10}\n",
      "0.38909468125996366                                                              \n",
      "{'activation_scale': 8.622500595000291, 'lambda_l1': 0.05502884358469466, 'lambda_l2': 3.501055455752659e-07, 'learning_rate': 0.00918265121223127, 'non': 18}\n",
      "0.36508061708836353                                                              \n",
      "{'activation_scale': 1.758816385013721, 'lambda_l1': 1.6996577901671958e-06, 'lambda_l2': 2.339882171077253e-05, 'learning_rate': 0.06862711136252612, 'non': 20}\n",
      "0.3787930827570803                                                               \n",
      "{'activation_scale': 5.395187756629524, 'lambda_l1': 0.005855469684775142, 'lambda_l2': 5.090457693030098e-05, 'learning_rate': 0.011268689922344937, 'non': 12}\n",
      "31.244587810739546                                                                \n",
      "{'activation_scale': 4.350864302684714, 'lambda_l1': 0.0016159040592944789, 'lambda_l2': 0.002700745856563716, 'learning_rate': 3.7109981359689854e-05, 'non': 10}\n",
      "0.38965800350171625                                                               \n",
      "{'activation_scale': 8.761875348127989, 'lambda_l1': 0.06810208712901768, 'lambda_l2': 0.012936873362479744, 'learning_rate': 0.010678118927173167, 'non': 14}\n",
      "33.11958072800072                                                                 \n",
      "{'activation_scale': 8.651786018104161, 'lambda_l1': 0.0006133841566356405, 'lambda_l2': 0.0005496169266334058, 'learning_rate': 1.8408216240834298e-05, 'non': 16}\n",
      "0.5812981953414489                                                                \n",
      "{'activation_scale': 0.6029979388946954, 'lambda_l1': 4.400888165456684e-06, 'lambda_l2': 0.004919573115570768, 'learning_rate': 0.0026358399005260392, 'non': 12}\n",
      "0.46530474688802015                                                               \n",
      "{'activation_scale': 6.126633375268467, 'lambda_l1': 0.0001536404768766214, 'lambda_l2': 1.1918024768563145e-07, 'learning_rate': 0.0009487893854567995, 'non': 10}\n",
      "0.5099222489549533                                                                \n",
      "{'activation_scale': 5.614603521426, 'lambda_l1': 0.0002174718954839058, 'lambda_l2': 2.6629440200209706e-07, 'learning_rate': 0.0008211903283234022, 'non': 18}\n",
      "0.8063743013697132                                                                \n",
      "{'activation_scale': 7.687045551623754, 'lambda_l1': 0.0032431769801183697, 'lambda_l2': 0.0023579047959915076, 'learning_rate': 0.0005645317653745055, 'non': 12}\n",
      "35.41010517345832                                                                 \n",
      "{'activation_scale': 0.32730531475184244, 'lambda_l1': 0.0003765495388318006, 'lambda_l2': 0.010450022517050015, 'learning_rate': 7.241466112587094e-05, 'non': 18}\n",
      "0.3708418773765372                                                                \n",
      "{'activation_scale': 3.1590930048144377, 'lambda_l1': 0.0001091501922127393, 'lambda_l2': 1.3621584228342358e-06, 'learning_rate': 0.09986524232562535, 'non': 16}\n",
      "7.336005442003242                                                                 \n",
      "{'activation_scale': 1.513173771737759, 'lambda_l1': 0.020930822467331422, 'lambda_l2': 0.000539977775131035, 'learning_rate': 0.0003197952589703129, 'non': 16}\n",
      "0.3728040097028614                                                                \n",
      "{'activation_scale': 9.943080648650243, 'lambda_l1': 1.374022890954621e-07, 'lambda_l2': 8.21653098907802e-06, 'learning_rate': 0.08107254119453687, 'non': 20}\n",
      "0.3860517817809148                                                                \n",
      "{'activation_scale': 6.592603997472855, 'lambda_l1': 7.245116839716833e-07, 'lambda_l2': 0.07400294143370655, 'learning_rate': 0.03684766595276256, 'non': 8}\n",
      "0.39213757780423697                                                               \n",
      "{'activation_scale': 2.371183637956301, 'lambda_l1': 1.3169945969530643e-06, 'lambda_l2': 6.684331704692461e-06, 'learning_rate': 0.09402008752091295, 'non': 14}\n",
      "0.386005221773543                                                                 \n",
      "{'activation_scale': 9.656099904866956, 'lambda_l1': 1.5116141337814974e-05, 'lambda_l2': 1.2990431599922549e-05, 'learning_rate': 0.01775598980997766, 'non': 8}\n",
      "0.3927941680568655                                                                \n",
      "{'activation_scale': 3.676210218001807, 'lambda_l1': 1.1614846910864807e-07, 'lambda_l2': 8.924183879269232e-05, 'learning_rate': 0.004048798845641544, 'non': 20}\n",
      "0.37539018700270493                                                               \n",
      "{'activation_scale': 2.5208273066347946, 'lambda_l1': 2.0651419971571853e-06, 'lambda_l2': 1.8949362226800662e-05, 'learning_rate': 0.02273704847169518, 'non': 14}\n",
      "0.3816324724527037                                                                \n",
      "{'activation_scale': 7.2073902269471395, 'lambda_l1': 6.027637977814367e-07, 'lambda_l2': 1.0599374051735387e-07, 'learning_rate': 0.005538792778370405, 'non': 12}\n",
      "0.36557949737701817                                                               \n",
      "{'activation_scale': 0.8137836580094059, 'lambda_l1': 3.327897783174977e-05, 'lambda_l2': 5.846384575772877e-07, 'learning_rate': 0.05467029960886046, 'non': 14}\n",
      "0.37755709274330324                                                               \n",
      "{'activation_scale': 0.8432880735191144, 'lambda_l1': 1.8300949151837663e-05, 'lambda_l2': 2.9410457847513207e-06, 'learning_rate': 0.01931690583618432, 'non': 20}\n",
      "0.4370812336463426                                                                \n",
      "{'activation_scale': 1.607933328986594, 'lambda_l1': 4.79239703648693e-05, 'lambda_l2': 0.00015098324844463746, 'learning_rate': 0.0018014564454054136, 'non': 20}\n",
      "7.398586247667091                                                                 \n",
      "{'activation_scale': 2.255935146273713, 'lambda_l1': 2.279379121403907e-06, 'lambda_l2': 3.351819118913615e-05, 'learning_rate': 0.00022731848224736182, 'non': 18}\n",
      "0.37136654318572687                                                               \n",
      "{'activation_scale': 4.282718336786566, 'lambda_l1': 5.372139561181506e-05, 'lambda_l2': 3.7492742579457986e-06, 'learning_rate': 0.059836194510734284, 'non': 14}\n",
      "0.40820296463506445                                                               \n",
      "{'activation_scale': 1.0359057356777592, 'lambda_l1': 5.063722395400856e-06, 'lambda_l2': 6.982170842394e-07, 'learning_rate': 0.005509003289062925, 'non': 8}\n",
      "0.39079450627943946                                                               \n",
      "{'activation_scale': 0.22975593462040322, 'lambda_l1': 1.8556159504874457e-05, 'lambda_l2': 0.0001497677741590529, 'learning_rate': 0.023877041662293496, 'non': 16}\n",
      "0.3710847339395814                                                                \n",
      "{'activation_scale': 3.277311081135731, 'lambda_l1': 2.5264209050442223e-07, 'lambda_l2': 5.011369457524445e-06, 'learning_rate': 0.05888546802340308, 'non': 18}\n",
      "0.589055207533137                                                                 \n",
      "{'activation_scale': 2.0109075899585926, 'lambda_l1': 8.922049363530392e-06, 'lambda_l2': 6.64161655540196e-07, 'learning_rate': 0.0012710624588379794, 'non': 16}\n",
      "0.3853048457434845                                                                \n",
      "{'activation_scale': 2.8394509839106554, 'lambda_l1': 0.0006559741733119487, 'lambda_l2': 2.0720638043689736e-07, 'learning_rate': 0.006518807990511013, 'non': 20}\n",
      "38.825068983367906                                                                \n",
      "{'activation_scale': 4.061821709173086, 'lambda_l1': 5.381702623618858e-07, 'lambda_l2': 2.2594276482725207e-05, 'learning_rate': 1.103729112806092e-05, 'non': 14}\n",
      "0.38308309514181593                                                               \n",
      "{'activation_scale': 0.1728662631732808, 'lambda_l1': 4.60789876707297e-05, 'lambda_l2': 5.462687548397668e-05, 'learning_rate': 0.03432950910661001, 'non': 10}\n",
      "0.40094091205769944                                                               \n",
      "{'activation_scale': 4.894440269714134, 'lambda_l1': 0.010079200475531026, 'lambda_l2': 1.3121502519530564e-06, 'learning_rate': 0.01332921212520812, 'non': 18}\n",
      "0.4240231579487861                                                                \n",
      "{'activation_scale': 1.819198169165039, 'lambda_l1': 4.794358128035881e-06, 'lambda_l2': 0.0012206685047998656, 'learning_rate': 0.0034107836812442213, 'non': 8}\n",
      "10.625737423974334                                                                \n",
      "{'activation_scale': 2.812342164213225, 'lambda_l1': 1.1243577447837482e-06, 'lambda_l2': 0.03555689306759422, 'learning_rate': 0.0001497711622690306, 'non': 12}\n",
      "5.6432745537743765                                                                \n",
      "{'activation_scale': 1.087590324742894, 'lambda_l1': 2.5509640709274324e-05, 'lambda_l2': 0.0003370576736071007, 'learning_rate': 0.0004692498095989188, 'non': 16}\n",
      "0.4113970943342876                                                                \n",
      "{'activation_scale': 3.9158967776558917, 'lambda_l1': 2.415404239081598e-07, 'lambda_l2': 2.1097108141281886e-06, 'learning_rate': 0.0014905444441957365, 'non': 10}\n",
      "0.3835684512010753                                                                \n",
      "{'activation_scale': 5.413820249762555, 'lambda_l1': 0.0009485464892315254, 'lambda_l2': 5.161224437714782e-07, 'learning_rate': 0.007949729975754407, 'non': 18}\n",
      "0.3717928878736406                                                                \n",
      "{'activation_scale': 0.617859694467531, 'lambda_l1': 9.084933335837268e-06, 'lambda_l2': 1.007524144414737e-05, 'learning_rate': 0.04575762672520694, 'non': 12}\n",
      "35.39107402591383                                                                 \n",
      "{'activation_scale': 1.3123726233091597, 'lambda_l1': 0.00026332790038086464, 'lambda_l2': 1.4886645326944293e-07, 'learning_rate': 4.326423878756643e-05, 'non': 14}\n",
      "0.3767076834675591                                                                \n",
      "{'activation_scale': 4.489584487060701, 'lambda_l1': 8.490709728328278e-05, 'lambda_l2': 6.607058330581303e-05, 'learning_rate': 0.014175456277745168, 'non': 20}\n",
      "0.3761215737511883                                                                \n",
      "{'activation_scale': 3.248642708127602, 'lambda_l1': 2.9029706561917325e-06, 'lambda_l2': 0.0002352023469555548, 'learning_rate': 0.02652895630579227, 'non': 18}\n",
      "0.382994254279353                                                                 \n",
      "{'activation_scale': 0.5763471277763368, 'lambda_l1': 3.199385821027117e-05, 'lambda_l2': 2.827322048950582e-05, 'learning_rate': 0.07120848743162263, 'non': 10}\n",
      "100%|██████████| 50/50 [19:42<00:00, 23.64s/trial, best loss: 0.36508061708836353]\n",
      "Best hyperparameters: {'activation_scale': 1.758816385013721, 'lambda_l1': 1.6996577901671958e-06, 'lambda_l2': 2.339882171077253e-05, 'learning_rate': 0.06862711136252612, 'non': 20.0}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=tpe.suggest,\n",
    "                   max_evals=50,\n",
    "                   trials=trials)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df.to_csv(f'param_df{current_target}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_7888\\90643497.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"{current_target}_bestest_model.pth\")\n",
    "params1 = checkpoint['params']\n",
    "model = Net(\n",
    "    n_feature=8,\n",
    "    n_hidden=params1['non'],\n",
    "    n_output=1,\n",
    "    activation_fn=nn.SiLU,\n",
    "    activation_scale=params1['activation_scale']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13289397533459016"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(dataset['test_input'])\n",
    "mse(dataset['test_label'].numpy(), test_predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8792726361991697"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(dataset['test_label'].numpy(), test_predictions.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
